{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "from random import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "maxSeqLength = 200\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000\n",
    "numStories = 43000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv('fake.csv')\n",
    "print fake_df.columns\n",
    "fake_text = fake_df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_json('signalmedia-1m.jsonl',lines=True)\n",
    "print real_df.columns\n",
    "real_text = real_df['content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 50,000 random stories from the real dataset\n",
    "real_i = np.random.choice(len(real_text), size=30000, replace=False)\n",
    "real_text = real_text[real_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel():\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open('glove.6B.100d.txt','r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# glove word-vec dictionary\n",
    "glove = loadGloveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories x the first 200 words\n",
    "train_matrix = np.empty((numStories, maxSeqLength+1), dtype='object')\n",
    "story_index = 0\n",
    "for story in real_text:\n",
    "    words = np.empty((maxSeqLength+1), dtype='object')\n",
    "    indexCounter = 0\n",
    "    cleanedLine = cleanSentences(story)\n",
    "    split = cleanedLine.split()\n",
    "    if len(split)< maxSeqLength:\n",
    "        indexCounter = maxSeqLength-len(split)\n",
    "    else:\n",
    "        split = split[:maxSeqLength]\n",
    "    for word in split:\n",
    "        words[indexCounter] = word\n",
    "        indexCounter = indexCounter + 1\n",
    "    words[maxSeqLength]=\"REAL\"\n",
    "    train_matrix[story_index] = words\n",
    "    story_index+=1\n",
    "\n",
    "for story in fake_text:\n",
    "    words = np.empty((maxSeqLength+1), dtype='object')\n",
    "    indexCounter = 0\n",
    "    if type(story) is float:\n",
    "        story = str(story)\n",
    "    cleanedLine = cleanSentences(story)\n",
    "    split = cleanedLine.split()\n",
    "    if len(split)< maxSeqLength:\n",
    "        indexCounter = maxSeqLength-len(split)\n",
    "    else:\n",
    "        split = split[:maxSeqLength]\n",
    "    for word in split:\n",
    "        words[indexCounter] = word\n",
    "        indexCounter = indexCounter + 1\n",
    "    words[maxSeqLength]=\"Fake\"\n",
    "    train_matrix[story_index] = words\n",
    "    story_index+=1\n",
    "\n",
    "np.save('my_train_matrix', train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train_matrix.shape\n",
    "#matrix_data = np.load('train_matrix.npy')\n",
    "np.save('my_train_matrix', train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42999, 200)\n",
      "(42999,)\n"
     ]
    }
   ],
   "source": [
    "matrix_data = np.load('my_train_matrix.npy')\n",
    "np.random.shuffle(matrix_data)\n",
    "labels = matrix_data[:-1,-1]\n",
    "data = matrix_data[:-1,:-1]\n",
    "print data.shape\n",
    "print labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1917494, 300)\n"
     ]
    }
   ],
   "source": [
    "glove_vecs = pd.Series(glove)\n",
    "glove_vecs = np.array(list(glove_vecs.values))\n",
    "print glove_vecs.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ketyung' 'coolwaremax' 'pre-increment' 'talbe' 'seniro' 'talba'\n",
      " 'tuberoses' 'pudz' 'gainsbourg' 'shuncheng' 'sebokeng' 'groooovy'\n",
      " 'toiket' 'imagesgallery' 'airball' 'write/read' 'hiranuma' 'achla'\n",
      " 'urbanbaby' 'blasse' 'microdesign' 'newstandard' 'sidebars' 'buy-here'\n",
      " 'bertholet' 'flyingfungal'\n",
      " '-----------------------------------------------------------------------------------------------------------------------------------------'\n",
      " 'compettive' 'accpetance' 'benedikt' '11.05.2008' 'deloreans' 'xoda'\n",
      " 'gelbert' 'pre-vietnam' 'storiesin' 'sylves' 'sylver' '11.05.2007'\n",
      " '11.05.2006' 'sylven' 'downloadmoneyworld' 'delphyne' 'expeditionary'\n",
      " 'gelberg' '\\xc3\\xbaltimamente' 'majorsjob' 'shoppingfood' 'sites.a'\n",
      " 'water-course' 'tumtube.com' 'eyehole' 'sites.i' 'sidebar.' 'kletke'\n",
      " 'again-off' 'essix' 'ozment' 'hans-erik' 'opravdu' 'fluorite' 'jidaigeki'\n",
      " 'edgeways' 'embarressing' 'kleffman' 'sunrays' 'basolo' 'missesdressy'\n",
      " 'agencyfaqs' 'avengedsevenfold' '230k-dot' '.6705' 'italso' 'tothree'\n",
      " 'isa-95' '.6700' 'tagtooga' '93-79' '93-78' 'shippingas' 'baladi'\n",
      " 'shippingau' 'balado' 'assimilated' 'prenestina' 'balade' 'balady'\n",
      " 'fimbriate' '1bohdan1' 'fimbriata' 'prenestino' 'balads' 'dc2tog'\n",
      " 'assimilates' 'alakum' 'daviddavid' 'profileapr' 'rapid-prototyping'\n",
      " 'vasters' 'db-fr']\n"
     ]
    }
   ],
   "source": [
    "glove_keys=np.array(glove.keys(), dtype=object)\n",
    "print glove_keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42999, 200, 100)\n"
     ]
    }
   ],
   "source": [
    "def word2vec(word):\n",
    "    try:\n",
    "        return glove[word]\n",
    "    except KeyError:\n",
    "        return np.zeros((100))\n",
    "\n",
    "vec_data = [[word2vec(w) for w in story] for story in data]\n",
    "vec_data = np.array(vec_data)\n",
    "print vec_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(X):\n",
    "    if X == \"REAL\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    labels = map(to_binary, labels)\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), [int(l) for l in labels]] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(vec_data, binary_labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 1000\n",
    "time_size = 5\n",
    "class RNN_cell(object):\n",
    "\n",
    "    \"\"\"\n",
    "    RNN cell object which takes 3 arguments for initialization.\n",
    "    input_size = Input Vector size\n",
    "    hidden_layer_size = Hidden layer size\n",
    "    target_size = Output vector size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_size, target_size):\n",
    "\n",
    "        #Initialization of given values\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Weights for input and hidden tensor\n",
    "        self.Wx = tf.Variable(tf.zeros([self.input_size,self.hidden_layer_size]))\n",
    "        self.Wr = tf.Variable(tf.zeros([self.input_size,self.hidden_layer_size]))\n",
    "        self.Wz = tf.Variable(tf.zeros([self.input_size,self.hidden_layer_size]))\n",
    "        \n",
    "        self.br = tf.Variable(tf.truncated_normal([self.hidden_layer_size],mean=1))\n",
    "        self.bz = tf.Variable(tf.truncated_normal([self.hidden_layer_size],mean=1))\n",
    "        \n",
    "        self.Wh = tf.Variable(tf.zeros([self.hidden_layer_size,self.hidden_layer_size]))\n",
    "\n",
    "        \n",
    "        #Weights for output layer\n",
    "        self.Wo = tf.Variable(tf.truncated_normal([self.hidden_layer_size,self.target_size],mean=1,stddev=.01))\n",
    "        self.bo = tf.Variable(tf.truncated_normal([self.target_size],mean=1,stddev=.01))\n",
    "        # Placeholder for input vector with shape[batch, seq, embeddings]\n",
    "        self._inputs = tf.placeholder(tf.float32,\n",
    "                                      shape=[None, None, self.input_size],\n",
    "                                      name='inputs')\n",
    "\n",
    "        # Processing inputs to work with scan function\n",
    "        self.processed_input = process_batch_input_for_RNN(self._inputs)\n",
    "\n",
    "        '''\n",
    "        Initial hidden state's shape is [1,self.hidden_layer_size]\n",
    "        In First time stamp, we are doing dot product with weights to\n",
    "        get the shape of [batch_size, self.hidden_layer_size].\n",
    "        For this dot product tensorflow use broadcasting. But during\n",
    "        Back propagation a low level error occurs.\n",
    "        So to solve the problem it was needed to initialize initial\n",
    "        hiddden state of size [batch_size, self.hidden_layer_size].\n",
    "        So here is a little hack !!!! Getting the same shaped\n",
    "        initial hidden state of zeros.\n",
    "        '''\n",
    "\n",
    "        self.initial_hidden = self._inputs[:, 0, :]\n",
    "        self.initial_hidden = tf.matmul(\n",
    "            self.initial_hidden, tf.zeros([input_size, hidden_layer_size]))\n",
    "        \n",
    "        \n",
    "    #Function for GRU cell\n",
    "    def Gru(self, previous_hidden_state, x):\n",
    "        \"\"\"\n",
    "        GRU Equations\n",
    "        \"\"\"\n",
    "        z= tf.sigmoid(tf.matmul(x,self.Wz)+ self.bz)\n",
    "        r= tf.sigmoid(tf.matmul(x,self.Wr)+ self.br)\n",
    "        \n",
    "        h_= tf.tanh(tf.matmul(x,self.Wx) + tf.matmul(previous_hidden_state,self.Wh)*r)\n",
    "                    \n",
    "        \n",
    "        current_hidden_state = tf.multiply((1-z),h_) + tf.multiply(previous_hidden_state,z)\n",
    "        \n",
    "        return current_hidden_state     \n",
    "    \n",
    "    # Function for getting all hidden state.\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Iterates through time/ sequence to get all hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting all hidden state throuh time\n",
    "        all_hidden_states = tf.scan(self.Gru,\n",
    "                                    self.processed_input,\n",
    "                                    initializer=self.initial_hidden,\n",
    "                                    name='states')\n",
    "\n",
    "        return all_hidden_states\n",
    "\n",
    "    # Function to get output from a hidden layer\n",
    "    def get_output(self, hidden_state):\n",
    "        \"\"\"\n",
    "        This function takes hidden state and returns output\n",
    "        \"\"\"\n",
    "        output = tf.nn.relu(tf.matmul(hidden_state, self.Wo) + self.bo)\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Function for getting all output layers\n",
    "    def get_outputs(self):\n",
    "        \"\"\"\n",
    "        Iterating through hidden states to get outputs for all timestamp\n",
    "        \"\"\"\n",
    "        all_hidden_states = self.get_states()\n",
    "\n",
    "        all_outputs = tf.map_fn(self.get_output, all_hidden_states)\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "# Function to convert batch input data to use scan ops of tensorflow.\n",
    "def process_batch_input_for_RNN(batch_input):\n",
    "    \"\"\"\n",
    "    Process tensor of size [5,3,2] to [3,5,2]\n",
    "    \"\"\"\n",
    "    batch_input_ = tf.transpose(batch_input, perm=[2, 0, 1])\n",
    "    X = tf.transpose(batch_input_)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 30\n",
    "input_size = 100\n",
    "target_size = 2\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, target_size],name='inputs')\n",
    "#Initializing rnn object\n",
    "rnn=RNN_cell( input_size, hidden_layer_size, target_size)\n",
    "\n",
    "#Getting all outputs from rnn\n",
    "outputs = rnn.get_outputs()\n",
    "\n",
    "#Getting final output through indexing after reversing\n",
    "last_output = outputs[-1]\n",
    "\n",
    "#As rnn model output the final layer through Relu activation softmax is used for final output.\n",
    "output=tf.nn.softmax(last_output)\n",
    "\n",
    "#Computing the Cross Entropy loss \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
    "\n",
    "# Trainning with Adadelta Optimizer\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "\n",
    "#Calculatio of correct prediction and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'cost=', '0.622020185')\n",
      "('Epoch:', '0002', 'cost=', '0.612218618')\n",
      "('Epoch:', '0003', 'cost=', '0.598558247')\n",
      "('Epoch:', '0004', 'cost=', '0.506338596')\n",
      "('Epoch:', '0005', 'cost=', '0.491447955')\n",
      "('Epoch:', '0006', 'cost=', '0.484116822')\n",
      "('Epoch:', '0007', 'cost=', '0.481873631')\n",
      "('Epoch:', '0008', 'cost=', '0.475472540')\n",
      "('Epoch:', '0009', 'cost=', '0.471095741')\n",
      "('Epoch:', '0010', 'cost=', '0.466704607')\n",
      "('Epoch:', '0011', 'cost=', '0.463764578')\n",
      "('Epoch:', '0012', 'cost=', '0.460626662')\n",
      "('Epoch:', '0013', 'cost=', '0.457104266')\n",
      "('Epoch:', '0014', 'cost=', '0.449717164')\n",
      "('Epoch:', '0015', 'cost=', '0.461616725')\n",
      "('Epoch:', '0016', 'cost=', '0.446820378')\n",
      "('Epoch:', '0017', 'cost=', '0.441348851')\n",
      "('Epoch:', '0018', 'cost=', '0.438674033')\n",
      "('Epoch:', '0019', 'cost=', '0.436278164')\n",
      "('Epoch:', '0020', 'cost=', '0.437655389')\n",
      "('Epoch:', '0021', 'cost=', '0.436002493')\n",
      "('Epoch:', '0022', 'cost=', '0.435992122')\n",
      "('Epoch:', '0023', 'cost=', '0.435400903')\n",
      "('Epoch:', '0024', 'cost=', '0.432023138')\n",
      "('Epoch:', '0025', 'cost=', '0.432683975')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-51ff21474025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         _, c = sess.run([train_step, cross_entropy], \n\u001b[0;32m---> 15\u001b[0;31m                         feed_dict={rnn._inputs:batch_x, y:batch_y})\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnew_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_epochs = 500\n",
    "display_step = 1\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    new_cost = 0.0\n",
    "    total_batch = int(len(x_train) / batch_size)\n",
    "    x_batches = np.array_split(x_train, total_batch)\n",
    "    y_batches = np.array_split(y_train, total_batch)\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "        _, c = sess.run([train_step, cross_entropy], \n",
    "                        feed_dict={rnn._inputs:batch_x, y:batch_y})\n",
    "        new_cost = c\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "            \"{:.9f}\".format(new_cost))\n",
    "print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5fb984e8f74f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start session.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Start session.\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        new_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y\n",
    "                            })\n",
    "            new_cost = c\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(new_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: X_val, y: y_val}))\n",
    "    \n",
    "    #Now, save the graph\n",
    "    predict = tf.argmax(pred, 1)\n",
    "    test_pred = predict.eval({x: test})\n",
    "    f = open('results.txt','a')\n",
    "    \n",
    "    print(test_pred.shape)\n",
    "    for prediction in test_pred:\n",
    "        f.write(str(prediction) + '\\n')\n",
    "    f.close()\n",
    "    \n",
    "    df = pd.DataFrame(test_pred)\n",
    "    df.to_csv(\"results.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
